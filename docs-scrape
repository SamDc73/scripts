#!/bin/bash

recursive_scrape() {
    local base_url="$1"
    local output_dir="$2"

    if [ -z "$base_url" ] || [ -z "$output_dir" ]; then
        echo "Usage: $0 <base_url> <output_dir>"
        exit 1
    fi

    echo "Starting to scrape: $base_url"
    echo "Output directory: $output_dir"

    # Create temp and output directories
    local temp_dir="temp_download"
    mkdir -p "$temp_dir"
    mkdir -p "$output_dir"

    # Download the whole site
    echo "Downloading site..."
    wget \
        --recursive \
        --no-clobber \
        --page-requisites \
        --html-extension \
        --convert-links \
        --restrict-file-names=windows \
        --domains $(echo "$base_url" | awk -F/ '{print $3}') \
        --no-parent \
        --directory-prefix="$temp_dir" \
        "$base_url"

    # Convert each HTML to clean MD
    echo "Converting to Markdown..."
    find "$temp_dir" -name "*.html" | while read file; do
        # Create relative output path
        relative_path=${file#"$temp_dir/"}
        output_file="$output_dir/${relative_path%.html}.md"
        
        # Create output directory structure
        mkdir -p "$(dirname "$output_file")"
        
        echo "Converting: $relative_path"
        
        # Convert to clean markdown
        pandoc "$file" \
            -f html \
            -t markdown \
            --wrap=none \
            --standalone \
            --strip-comments \
            --no-highlight \
            | sed -e '/^:::.*$/d' \
                  -e '/^<.*>$/d' \
                  -e '/^{.*}$/d' \
                  -e 's/\[\!\[\]\(data:image\/svg.*\)\]//g' \
            > "$output_file"
    done

    # Cleanup
    echo "Cleaning up..."
    rm -rf "$temp_dir"

    echo "Done! Files saved in $output_dir"
}

# Run the script
recursive_scrape "$1" "$2"
